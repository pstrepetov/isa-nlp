{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzZyVLAtCZ-G"
   },
   "source": [
    "## Коррекция опечаток\n",
    "\n",
    "([Задание для семинара](#scrollTo=17IUm4LgTykL), [Домашнее задание](#scrollTo=vHh31ssMCZ-w&line=12&uniqifier=1))\n",
    "\n",
    "При наборе текста автор может допустить опечатки, орфографические  и грамматические ошибки в словах. У специальных терминов также может  существовать несколько допустимых  форм написания. Рассмотрим задачу исправления  ошибочного написания слова, которое  можно обнаружить сверкой\n",
    "со словарем.\n",
    "\n",
    "Рассмотрим, какие методы используются для коррекции опечаток, какие мультиязычные и русскоязычные библиотеки можно использовать.\n",
    "\n",
    "ПРИМЕР (из [Dialogue evaluation 2016](http://www.dialog-21.ru/en/evaluation/2016/spelling_correction/) ):\n",
    "\n",
    "неправильно\n",
    ">вот в инете откапал такую интеерсную статейку предлагаю вашему внимани\n",
    "\n",
    "правильно\n",
    ">вот в инете **откопал** такую **интересную** статейку предлагаю вашему **вниманию**\n",
    "\n",
    "\n",
    "Пусть слова из обрабатываемого текста необходимо сверить с некоторым словарем.  Существует два основных способа сверки:\n",
    " 1. Сгенерировать множество модификаций слова и найти эти модификации в словаре; \n",
    " 2. Найти в словаре наиболее похожие токены на основе некоторое метрики сходства (расстояние редактирования, расстояние Хэмминга, близость векторов и т.д.).\n",
    "\n",
    "Из найденного набора вариантов для замены неправильного слова необходимо найти  наиболее вероятный для данного контекста (например, на основе частотности словарного  слова в языке или синтаксиса)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17IUm4LgTykL"
   },
   "source": [
    "### 1. Базовый способ\n",
    "\n",
    "**Задание (Семинар, 2 балла):** Дополните код функции, возвращающей самый частотный из наиболее похожих на данное слово слов словаря. \n",
    "\n",
    "Простейший подход к исправлению слов: находим в словаре наиболее похожие токены, из них выбираем самый вероятный (например, самый частотный) для замены некорректно напечатанного.\n",
    "\n",
    "В качестве примера возьмем какой-нибудь готовый частотный словарь для русского языка, например, [такой](https://github.com/Baksalyar/mc.hertzbeat.ru-Frequency-Dictionaries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "D05YFq_-CZ-I"
   },
   "outputs": [],
   "source": [
    "! wget -q https://github.com/Baksalyar/mc.hertzbeat.ru-Frequency-Dictionaries/raw/master/mc.hertzbeat.ru_frequency_dict.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_XLfMh-SCZ-L"
   },
   "outputs": [],
   "source": [
    "freq_dict = []\n",
    "\n",
    "with open(\"mc.hertzbeat.ru_frequency_dict.txt\", \"r\") as f:\n",
    "    for string in f:\n",
    "        freq_dict.append((string.split()[0], int(string.split()[1].strip())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "executionInfo": {
     "elapsed": 4100,
     "status": "ok",
     "timestamp": 1600866512730,
     "user": {
      "displayName": "Elena Chistova",
      "photoUrl": "",
      "userId": "12812817226965674873"
     },
     "user_tz": -180
    },
    "id": "285M-zTvCZ-O",
    "outputId": "31ac5cad-3dac-4e0b-e5eb-01916cf60cec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('в', 1546309),\n",
       " ('и', 1124915),\n",
       " ('на', 779113),\n",
       " ('не', 471559),\n",
       " ('с', 467340),\n",
       " ('что', 441934),\n",
       " ('по', 328627),\n",
       " ('для', 225411),\n",
       " ('а', 206812),\n",
       " ('из', 204946)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dict[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQO6jfbXCZ-S"
   },
   "source": [
    "Базовая метрика близости токенов: **Расстояние Дамерау-Левенштейна** или редакционное расстояние с учетом транспозиции.\n",
    "\n",
    "Эта функция есть в ``nltk``: ``nltk.metrics.edit_distance``. Код приведен для ясности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PC3CKXchCZ-S"
   },
   "outputs": [],
   "source": [
    "def damerau_levenshtein_distance(s1, s2, transposition=True):\n",
    "    d = {}\n",
    "    lenstr1 = len(s1)\n",
    "    lenstr2 = len(s2)\n",
    "    for i in range(-1,lenstr1+1):\n",
    "        d[(i,-1)] = i+1\n",
    "    for j in range(-1,lenstr2+1):\n",
    "        d[(-1,j)] = j+1\n",
    " \n",
    "    for i in range(lenstr1):\n",
    "        for j in range(lenstr2):\n",
    "            if s1[i] == s2[j]:\n",
    "                cost = 0\n",
    "            else:\n",
    "                cost = 1\n",
    "            d[(i,j)] = min(\n",
    "                           d[(i-1,j)] + 1, # deletion\n",
    "                           d[(i,j-1)] + 1, # insertion\n",
    "                           d[(i-1,j-1)] + cost, # substitution\n",
    "                          )\n",
    "            if transposition:\n",
    "                if i and j and s1[i]==s2[j-1] and s1[i-1] == s2[j]:\n",
    "                    d[(i,j)] = min (d[(i,j)], d[i-2,j-2] + cost) # transposition\n",
    " \n",
    "    return d[lenstr1-1,lenstr2-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CwG4FaRoCZ-V"
   },
   "outputs": [],
   "source": [
    "wrong_text, true_text = \"вот в инете откапал такую интеерсную статейку предлагаю вашему внимани\", \"вот в инете откопал такую интересную статейку предлагаю вашему вниманию\"\n",
    "wrong, true = wrong_text.split(), true_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "executionInfo": {
     "elapsed": 648,
     "status": "ok",
     "timestamp": 1600866514594,
     "user": {
      "displayName": "Elena Chistova",
      "photoUrl": "",
      "userId": "12812817226965674873"
     },
     "user_tz": -180
    },
    "id": "gyEwo4sDCZ-Y",
    "outputId": "396725a8-a14b-4618-82f2-6a52a58a0db9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein distance between откапал and откопал = 1\n",
      "Levenshtein distance between интеерсную and интересную = 2\n",
      "Levenshtein distance between внимани and вниманию = 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(true)):\n",
    "    if wrong[i] != true[i]:\n",
    "        dld = damerau_levenshtein_distance(wrong[i], true[i], transposition=False)\n",
    "        print(f\"Levenshtein distance between {wrong[i]} and {true[i]} = {dld}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "executionInfo": {
     "elapsed": 1368,
     "status": "ok",
     "timestamp": 1600866515521,
     "user": {
      "displayName": "Elena Chistova",
      "photoUrl": "",
      "userId": "12812817226965674873"
     },
     "user_tz": -180
    },
    "id": "e8Y_DOMCCZ-b",
    "outputId": "52d19773-f424-424d-e3d4-4f455e7134ad",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Damerau–Levenshtein distance between откапал and откопал = 1\n",
      "Damerau–Levenshtein distance between интеерсную and интересную = 1\n",
      "Damerau–Levenshtein distance between внимани and вниманию = 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(true)):\n",
    "    if wrong[i] != true[i]:\n",
    "        dld = damerau_levenshtein_distance(wrong[i], true[i], transposition=True)\n",
    "        print(f\"Damerau–Levenshtein distance between {wrong[i]} and {true[i]} = {dld}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BmBCTxRPCZ-e"
   },
   "outputs": [],
   "source": [
    "def in_dict(token):\n",
    "    for s in freq_dict:\n",
    "        if s[0] == token:\n",
    "            return token\n",
    "    \n",
    "def dummy_spellcorrection(token, max_dist=2):\n",
    "    if in_dict(token):\n",
    "        return token\n",
    "    #########################\n",
    "    # ToDO: впишите функцию для нахождения ближайшего слова в словаре \n",
    "    # (похожим будем считать с расстоянием до max_dist включительно)\n",
    "    # среди словарных слов с одинаковым расстоянием выбирайте наиболее частотное\n",
    "    # помните, что словарь упорядочен по частоте\n",
    "    \n",
    "    ans = token #если не сможем найти похожее слово, то выведем исходный токен\n",
    "    min_dist = 1 #если токен не в словаре, то для него минимальное расстояние Д-Л будет равно 1\n",
    "    dist = max_dist + 1 #значение, куда будем записывать текущее расстояние Д-Л\n",
    "    len_t = len(token)\n",
    "    \n",
    "    for s in freq_dict:\n",
    "        if (abs(len(s[0]) - len_t) <= max_dist): #разница в длине слова и токена не должна превышать max_dist\n",
    "            dld = damerau_levenshtein_distance(s[0], token, transposition=True)\n",
    "            if (dld <= max_dist and dld < dist):\n",
    "                ans = s[0]\n",
    "                dist = dld\n",
    "            if (dist == min_dist):\n",
    "                return ans\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "executionInfo": {
     "elapsed": 781,
     "status": "ok",
     "timestamp": 1600866516272,
     "user": {
      "displayName": "Elena Chistova",
      "photoUrl": "",
      "userId": "12812817226965674873"
     },
     "user_tz": -180
    },
    "id": "K85lSi6CCZ-h",
    "outputId": "b970419f-dc9a-4660-c92e-d4ab757696a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 906 ms, sys: 0 ns, total: 906 ms\n",
      "Wall time: 930 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['вот',\n",
       " 'в',\n",
       " 'инете',\n",
       " 'отказал',\n",
       " 'такую',\n",
       " 'интересную',\n",
       " 'статейку',\n",
       " 'предлагаю',\n",
       " 'вашему',\n",
       " 'внимание']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "[dummy_spellcorrection(word) for word in wrong]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrtIive-CZ-k"
   },
   "source": [
    "### 2. Генеративный способ\n",
    "\n",
    "Вместо того, чтобы сверять слово, не найденное в словаре, со словарными путем нахождения редакционного или другого расстояния, генерируем для него различные модификации и сравниваем их со словарными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "iIVQhclBCZ-k"
   },
   "outputs": [],
   "source": [
    "def norvig_spellcorrection(token): \n",
    "    if in_dict(token):\n",
    "        return token\n",
    "    \n",
    "    return max(candidates(token), key=lambda s: s[1])\n",
    "\n",
    "def candidates(word): \n",
    "    \"Генерируем кандидатов на исправление\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"Выбираем слова, которые есть в корпусе\"\n",
    "    return set(word for word in words if in_dict(word))\n",
    "\n",
    "def edits1(word):\n",
    "    \"Создаем кандидатов, которые отличаются на одну букву\"\n",
    "    letters    = 'йцукенгшщзхъфывапролджэячсмитьбюё'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"Создаем кандидатов, которые отличаются на две буквы\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "executionInfo": {
     "elapsed": 40172,
     "status": "ok",
     "timestamp": 1600866561665,
     "user": {
      "displayName": "Elena Chistova",
      "photoUrl": "",
      "userId": "12812817226965674873"
     },
     "user_tz": -180
    },
    "id": "qou8buF3CZ-n",
    "outputId": "d5d75d08-83a6-462e-f7b7-59611cccad73",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.4 s, sys: 31.2 ms, total: 33.4 s\n",
      "Wall time: 33.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['вот',\n",
       " 'в',\n",
       " 'инете',\n",
       " 'откопал',\n",
       " 'такую',\n",
       " 'интересную',\n",
       " 'статейку',\n",
       " 'предлагаю',\n",
       " 'вашему',\n",
       " 'внимания']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "[norvig_spellcorrection(word) for word in wrong]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bsmshAB-CZ-q"
   },
   "source": [
    "### Одно из готовых решений: JamSpell \n",
    "\n",
    "Учитывает контекст. Как это реализовано, можно почитать у автора: https://habr.com/ru/post/346618/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "xv8_8Mn2Dxn5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\r\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\r\n"
     ]
    }
   ],
   "source": [
    "! apt install swig3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "QKQcTZZYE8NN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jamspell\n",
      "  Downloading jamspell-0.0.12.tar.gz (174 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.3/174.3 kB\u001b[0m \u001b[31m215.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: jamspell\n",
      "  Building wheel for jamspell (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[52 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m /home/peter/p/tf-demo/lib/python3.9/site-packages/setuptools/dist.py:771: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead\n",
      "  \u001b[31m   \u001b[0m   warnings.warn(\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m building '_jamspell' extension\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-639frv8o/jamspell_731ed75ae0434d8b8df54d7b941e98c7/setup.py\", line 55, in <module>\n",
      "  \u001b[31m   \u001b[0m     setup(\n",
      "  \u001b[31m   \u001b[0m   File \"/home/peter/p/tf-demo/lib/python3.9/site-packages/setuptools/__init__.py\", line 87, in setup\n",
      "  \u001b[31m   \u001b[0m     return distutils.core.setup(**attrs)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/peter/p/tf-demo/lib/python3.9/site-packages/setuptools/_distutils/core.py\", line 185, in setup\n",
      "  \u001b[31m   \u001b[0m     return run_commands(dist)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/peter/p/tf-demo/lib/python3.9/site-packages/setuptools/_distutils/core.py\", line 201, in run_commands\n",
      "  \u001b[31m   \u001b[0m     dist.run_commands()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/peter/p/tf-demo/lib/python3.9/site-packages/setuptools/_distutils/dist.py\", line 973, in run_commands\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/peter/p/tf-demo/lib/python3.9/site-packages/setuptools/dist.py\", line 1217, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/peter/p/tf-demo/lib/python3.9/site-packages/setuptools/_distutils/dist.py\", line 992, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/peter/p/tf-demo/lib/python3.9/site-packages/wheel/bdist_wheel.py\", line 299, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command('build')\n",
      "  \u001b[31m   \u001b[0m   File \"/home/peter/p/tf-demo/lib/python3.9/site-packages/setuptools/_distutils/cmd.py\", line 319, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/peter/p/tf-demo/lib/python3.9/site-packages/setuptools/dist.py\", line 1217, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/peter/p/tf-demo/lib/python3.9/site-packages/setuptools/_distutils/dist.py\", line 992, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-639frv8o/jamspell_731ed75ae0434d8b8df54d7b941e98c7/setup.py\", line 37, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command('build_ext')\n",
      "  \u001b[31m   \u001b[0m   File \"/home/peter/p/tf-demo/lib/python3.9/site-packages/setuptools/_distutils/cmd.py\", line 319, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/peter/p/tf-demo/lib/python3.9/site-packages/setuptools/dist.py\", line 1217, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/peter/p/tf-demo/lib/python3.9/site-packages/setuptools/_distutils/dist.py\", line 992, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/peter/p/tf-demo/lib/python3.9/site-packages/setuptools/_distutils/command/build_ext.py\", line 346, in run\n",
      "  \u001b[31m   \u001b[0m     self.build_extensions()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/peter/p/tf-demo/lib/python3.9/site-packages/setuptools/_distutils/command/build_ext.py\", line 466, in build_extensions\n",
      "  \u001b[31m   \u001b[0m     self._build_extensions_serial()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/peter/p/tf-demo/lib/python3.9/site-packages/setuptools/_distutils/command/build_ext.py\", line 492, in _build_extensions_serial\n",
      "  \u001b[31m   \u001b[0m     self.build_extension(ext)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/peter/p/tf-demo/lib/python3.9/site-packages/setuptools/_distutils/command/build_ext.py\", line 525, in build_extension\n",
      "  \u001b[31m   \u001b[0m     sources = self.swig_sources(sources, ext)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/peter/p/tf-demo/lib/python3.9/site-packages/setuptools/_distutils/command/build_ext.py\", line 623, in swig_sources\n",
      "  \u001b[31m   \u001b[0m     swig = self.swig or self.find_swig()\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-639frv8o/jamspell_731ed75ae0434d8b8df54d7b941e98c7/setup.py\", line 49, in find_swig\n",
      "  \u001b[31m   \u001b[0m     assert swigBinary is not None\n",
      "  \u001b[31m   \u001b[0m AssertionError\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for jamspell\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for jamspell\n",
      "Failed to build jamspell\n",
      "Installing collected packages: jamspell\n",
      "  Running setup.py install for jamspell ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mRunning setup.py install for jamspell\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[45 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m /home/peter/p/tf-demo/lib/python3.9/site-packages/setuptools/dist.py:771: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead\n",
      "  \u001b[31m   \u001b[0m   warnings.warn(\n",
      "  \u001b[31m   \u001b[0m running install\n",
      "  \u001b[31m   \u001b[0m /home/peter/p/tf-demo/lib/python3.9/site-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  \u001b[31m   \u001b[0m   warnings.warn(\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m building '_jamspell' extension\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-639frv8o/jamspell_731ed75ae0434d8b8df54d7b941e98c7/setup.py\", line 55, in <module>\n",
      "  \u001b[31m   \u001b[0m     setup(\n",
      "  \u001b[31m   \u001b[0m   File \"/home/peter/p/tf-demo/lib/python3.9/site-packages/setuptools/__init__.py\", line 87, in setup\n",
      "  \u001b[31m   \u001b[0m     return distutils.core.setup(**attrs)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/peter/p/tf-demo/lib/python3.9/site-packages/setuptools/_distutils/core.py\", line 185, in setup\n",
      "  \u001b[31m   \u001b[0m     return run_commands(dist)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/peter/p/tf-demo/lib/python3.9/site-packages/setuptools/_distutils/core.py\", line 201, in run_commands\n",
      "  \u001b[31m   \u001b[0m     dist.run_commands()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/peter/p/tf-demo/lib/python3.9/site-packages/setuptools/_distutils/dist.py\", line 973, in run_commands\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/peter/p/tf-demo/lib/python3.9/site-packages/setuptools/dist.py\", line 1217, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/peter/p/tf-demo/lib/python3.9/site-packages/setuptools/_distutils/dist.py\", line 992, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-639frv8o/jamspell_731ed75ae0434d8b8df54d7b941e98c7/setup.py\", line 43, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command('build_ext')\n",
      "  \u001b[31m   \u001b[0m   File \"/home/peter/p/tf-demo/lib/python3.9/site-packages/setuptools/_distutils/cmd.py\", line 319, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/peter/p/tf-demo/lib/python3.9/site-packages/setuptools/dist.py\", line 1217, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/peter/p/tf-demo/lib/python3.9/site-packages/setuptools/_distutils/dist.py\", line 992, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/peter/p/tf-demo/lib/python3.9/site-packages/setuptools/_distutils/command/build_ext.py\", line 346, in run\n",
      "  \u001b[31m   \u001b[0m     self.build_extensions()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/peter/p/tf-demo/lib/python3.9/site-packages/setuptools/_distutils/command/build_ext.py\", line 466, in build_extensions\n",
      "  \u001b[31m   \u001b[0m     self._build_extensions_serial()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/peter/p/tf-demo/lib/python3.9/site-packages/setuptools/_distutils/command/build_ext.py\", line 492, in _build_extensions_serial\n",
      "  \u001b[31m   \u001b[0m     self.build_extension(ext)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/peter/p/tf-demo/lib/python3.9/site-packages/setuptools/_distutils/command/build_ext.py\", line 525, in build_extension\n",
      "  \u001b[31m   \u001b[0m     sources = self.swig_sources(sources, ext)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/peter/p/tf-demo/lib/python3.9/site-packages/setuptools/_distutils/command/build_ext.py\", line 623, in swig_sources\n",
      "  \u001b[31m   \u001b[0m     swig = self.swig or self.find_swig()\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-639frv8o/jamspell_731ed75ae0434d8b8df54d7b941e98c7/setup.py\", line 49, in find_swig\n",
      "  \u001b[31m   \u001b[0m     assert swigBinary is not None\n",
      "  \u001b[31m   \u001b[0m AssertionError\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mlegacy-install-failure\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while trying to install package.\n",
      "\u001b[31m╰─>\u001b[0m jamspell\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for output from the failure.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "! pip install -U jamspell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "TNwctYfqCZ-q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ru_small.bin\r\n"
     ]
    }
   ],
   "source": [
    "! wget -q https://github.com/bakwc/JamSpell-models/raw/master/ru.tar.gz && tar -xvf ru.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 36023,
     "status": "ok",
     "timestamp": 1600866612207,
     "user": {
      "displayName": "Elena Chistova",
      "photoUrl": "",
      "userId": "12812817226965674873"
     },
     "user_tz": -180
    },
    "id": "vt0QeLKXCZ-t",
    "outputId": "f8fa1cd5-772b-417d-f2ec-a5faba1ddd83"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jamspell'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [46]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjamspell\u001b[39;00m\n\u001b[1;32m      3\u001b[0m corrector \u001b[38;5;241m=\u001b[39m jamspell\u001b[38;5;241m.\u001b[39mTSpellCorrector()\n\u001b[1;32m      4\u001b[0m corrector\u001b[38;5;241m.\u001b[39mLoadLangModel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mru_small.bin\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'jamspell'"
     ]
    }
   ],
   "source": [
    "import jamspell\n",
    "\n",
    "corrector = jamspell.TSpellCorrector()\n",
    "corrector.LoadLangModel('ru_small.bin')\n",
    "\n",
    "corrector.FixFragment(wrong_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "вот в инете откопал такую интересную статейку предлагаю вашему вниманию\n"
     ]
    }
   ],
   "source": [
    "#!pip install pyaspeller\n",
    "from pyaspeller import YandexSpeller\n",
    "speller = YandexSpeller()\n",
    "fixed = speller.spelled(wrong_text)\n",
    "print(fixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qt3WsirNCZ-w"
   },
   "source": [
    "[Здесь](http://docs.deeppavlov.ai/en/master/features/models/spelling_correction.html#comparison) можно посмотреть на текущее положение дел в коррекции опечаток на русском языке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHh31ssMCZ-w"
   },
   "source": [
    "## Домашнее задание (6 баллов)\n",
    "\n",
    "Расстояние Дамерау-Левенштейна, как и многие другие методы нахождения расстояния  между строками, реализовано в различных библиотеках Python. Однако даже реализации  на Cython работают слишком медленно, чтобы их было удобно использовать в прикладных  задачах с большими словарями.\n",
    "\n",
    "Часть словаря, которая точно не подходит в качестве кандидата для замены неверно написанного слова, можно отфильтровать, прибегнув к векторизации токенов. Если косинусная близость между символьными векторами двух слов небольшая, между ними априори нет смысла вычислять символьное расстояние – если мы ищем кандидата на замену слову \"карова\", \"пешеход\" и \"ясень\" явно не подойдут.\n",
    "\n",
    "Задача: \n",
    " 1. Реализовать метод векторизации токенов, используя информацию о содержащихся в них символьных n-граммах; \n",
    " 2. На примере одного предложения на русском языке с несколькими опечатками продемонстрировать время работы функции ``dummy_spellcorrection`` в исходном виде (с поиском по всему словарю) и с фильтрацией по близости векторов слов словаря к вектору слова с опечаткой.\n",
    "\n",
    "Решения в форме ноутбуков, где последняя ячейка содержит ответ с текстовым комментарием, присылайте на mipttextanalysis20@gmail.com (ссылкой на colab.research или файлом).\n",
    "\n",
    "Решения без штрафа (8 баллов максимум) принимаются до 12.00 29 сентября. Всё, что присылается не в срок, оценивается из максимума 4 балла. Работу, присланную в срок и оцененную не менее чем на 4 балла, можно доделать и досдать на максимум один раз.\n",
    "\n",
    "Пожалуйста, указывайте фамилию в названии блокнота. Комментарий к решению принимается на русском или английском языке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = []\n",
    "for s in freq_dict:\n",
    "    wordlist.append(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480092\n"
     ]
    }
   ],
   "source": [
    "wordlist_l = len(wordlist)\n",
    "print(wordlist_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['По', 'мере', 'того', 'как', 'раскрывались', 'перид', 'ней', 'фазисы', 'жызни', 'то', 'есть', 'чуства', 'она', 'зорко', 'наблюдала', 'явления', 'чутко', 'прислушивалась', 'к', 'голасу', 'своево', 'инстинкта', 'и', 'слехка', 'повиряла', 'с', 'нимногими', 'бывшими', 'у', 'ней', 'в', 'запассе', 'наблюдениями', 'и', 'шла', 'астарожно', 'пытая', 'ногой', 'почву', 'на', 'катторую', 'пристояло', 'ступить']\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "wrong_sentence = \"По мере того как раскрывались перид ней фазисы жызни то есть чуства она зорко наблюдала явления чутко прислушивалась к голасу своево инстинкта и слехка повиряла с нимногими бывшими у ней в запассе наблюдениями и шла астарожно пытая ногой почву на катторую пристояло ступить\"\n",
    "w_sent = wrong_sentence.split()\n",
    "len_wsent = len(w_sent)\n",
    "print(w_sent)\n",
    "print(len_wsent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(analyzer = 'char', ngram_range=(2,2))\n",
    "dictionary = cv.fit_transform(wordlist)\n",
    "sentence = cv.transform(w_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4507\n"
     ]
    }
   ],
   "source": [
    "print(len(dictionary[0].toarray()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.zeros((480092, 4507), np.int8)\n",
    "\n",
    "for i in range(0, 480092):\n",
    "    X[i] = dictionary[i].toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480092, 4507)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X[0][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_v = np.zeros((43,4507), np.int8)\n",
    "for i in range(0, 43):\n",
    "    sent_v[i] = sentence[i].toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(sent_v[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance as d\n",
    "def in_dict(token):\n",
    "    for s in freq_dict:\n",
    "        if s[0] == token:\n",
    "            return token\n",
    "            \n",
    "def dummy_spellcorrection2(token, token_v, max_dist=2, C=0.3):\n",
    "    if in_dict(token):\n",
    "        return token\n",
    "    \n",
    "    ans = token #если не сможем найти похожее слово, то выведем исходный токен\n",
    "    min_dist = 1 #если токен не в словаре, то для него минимальное расстояние Д-Л будет равно 1\n",
    "    dist = max_dist + 1 #значение, куда будем записывать текущее расстояние Д-Л\n",
    "    len_t = len(token)\n",
    "    i = 0\n",
    "    while i < wordlist_l:\n",
    "        if (abs(len(freq_dict[i][0]) - len_t) <= max_dist):\n",
    "            if (d.cosine(X[i], token_v) < C):\n",
    "                dld = damerau_levenshtein_distance(freq_dict[i][0], token, transposition=True)\n",
    "                if (dld <= max_dist and dld < dist):\n",
    "                    ans = freq_dict[i][0]\n",
    "                    dist = dld\n",
    "                if (dist == min_dist):\n",
    "                    return ans\n",
    "        i+=1\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/p/tf-demo/lib/python3.9/site-packages/scipy/spatial/distance.py:620: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['по', 'мере', 'того', 'как', 'раскрывались', 'перед', 'ней', 'оазисы', 'жизни', 'то', 'есть', 'чувства', 'она', 'зорко', 'наблюдала', 'явления', 'чутко', 'прислушивалась', 'к', 'голосу', 'своего', 'инстинкта', 'и', 'слегка', 'потеряла', 'с', 'немногими', 'бывшими', 'у', 'ней', 'в', 'запасе', 'наблюдениями', 'и', 'шла', 'осторожно', 'пытая', 'ногой', 'почву', 'на', 'которую', 'простояло', 'ступить']\n",
      "CPU times: user 53.6 s, sys: 3.34 s, total: 57 s\n",
      "Wall time: 59.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "i = 0\n",
    "l = len(w_sent)\n",
    "ans = []\n",
    "while i < l:\n",
    "    ans.append(dummy_spellcorrection2(w_sent[i], sent_v[i], 2, 0.6))\n",
    "    i+=1\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 11s, sys: 188 ms, total: 1min 12s\n",
      "Wall time: 1min 14s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['по',\n",
       " 'мере',\n",
       " 'того',\n",
       " 'как',\n",
       " 'раскрывались',\n",
       " 'перед',\n",
       " 'ней',\n",
       " 'оазисы',\n",
       " 'жизни',\n",
       " 'то',\n",
       " 'есть',\n",
       " 'чувства',\n",
       " 'она',\n",
       " 'зорко',\n",
       " 'наблюдала',\n",
       " 'явления',\n",
       " 'чутко',\n",
       " 'прислушивалась',\n",
       " 'к',\n",
       " 'голосу',\n",
       " 'своего',\n",
       " 'инстинкта',\n",
       " 'и',\n",
       " 'слегка',\n",
       " 'потеряла',\n",
       " 'с',\n",
       " 'немногими',\n",
       " 'бывшими',\n",
       " 'у',\n",
       " 'ней',\n",
       " 'в',\n",
       " 'запасе',\n",
       " 'наблюдениями',\n",
       " 'и',\n",
       " 'шла',\n",
       " 'осторожно',\n",
       " 'пытая',\n",
       " 'ногой',\n",
       " 'почву',\n",
       " 'на',\n",
       " 'которую',\n",
       " 'простояло',\n",
       " 'ступить']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "[dummy_spellcorrection(word, 2) for word in w_sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример для max_dist = 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['по', 'мере', 'того', 'как', 'раскрывались', 'перед', 'ней', 'оазисы', 'жизни', 'то', 'есть', 'чувства', 'она', 'зорко', 'наблюдала', 'явления', 'чутко', 'прислушивалась', 'к', 'голосу', 'своего', 'инстинкта', 'и', 'слегка', 'потеряла', 'с', 'немногими', 'бывшими', 'у', 'ней', 'в', 'запасе', 'наблюдениями', 'и', 'шла', 'осторожно', 'пытая', 'ногой', 'почву', 'на', 'которую', 'простояло', 'ступить']\n",
      "CPU times: user 1min 30s, sys: 9.2 s, total: 1min 39s\n",
      "Wall time: 1min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "i = 0\n",
    "l = len(w_sent)\n",
    "ans = []\n",
    "while i < l:\n",
    "    ans.append(dummy_spellcorrection2(w_sent[i], sent_v[i], 5, 0.6))\n",
    "    i+=1\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 54s, sys: 125 ms, total: 1min 54s\n",
      "Wall time: 1min 55s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['по',\n",
       " 'мере',\n",
       " 'того',\n",
       " 'как',\n",
       " 'раскрывались',\n",
       " 'перед',\n",
       " 'ней',\n",
       " 'оазисы',\n",
       " 'жизни',\n",
       " 'то',\n",
       " 'есть',\n",
       " 'чувства',\n",
       " 'она',\n",
       " 'зорко',\n",
       " 'наблюдала',\n",
       " 'явления',\n",
       " 'чутко',\n",
       " 'прислушивалась',\n",
       " 'к',\n",
       " 'голосу',\n",
       " 'своего',\n",
       " 'инстинкта',\n",
       " 'и',\n",
       " 'слегка',\n",
       " 'потеряла',\n",
       " 'с',\n",
       " 'немногими',\n",
       " 'бывшими',\n",
       " 'у',\n",
       " 'ней',\n",
       " 'в',\n",
       " 'запасе',\n",
       " 'наблюдениями',\n",
       " 'и',\n",
       " 'шла',\n",
       " 'осторожно',\n",
       " 'пытая',\n",
       " 'ногой',\n",
       " 'почву',\n",
       " 'на',\n",
       " 'которую',\n",
       " 'простояло',\n",
       " 'ступить']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "[dummy_spellcorrection(word, 5) for word in w_sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Комментарий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оригинальное предложение: По мере того, как раскрывались перед ней фазисы жизни, то есть чувства, она зорко наблюдала явления, чутко прислушивалась к голосу своего инстинкта и слегка поверяла с немногими, бывшими у ней в запасе наблюдениями, и шла осторожно, пытая ногой почву, на которую предстояло ступить. (И. А Гончаров \"Обломов\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Векторизацию я выполнил при помощи CountVectorizer для символьных биграмм, поскольку размер векторов для триграмм и т.д. становится слишком большим. На данном предложении фильтрация по близости векторов слов словаря к вектору слова с опечаткой показала себя быстрее, чем поиск по всему словарю, но ускорение не очень большое (~10-15 сек при различных значениях max_dist). Предполагаю, что на бОльших предложениях и с более длинными словами ускорение может быть значительно выше."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2_spellcheck.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
